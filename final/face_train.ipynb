{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from load_dataset.ipynb\n",
      "{0: 'LinTzuTing', 1: 'abarnvbtwb', 2: 'Theodora', 3: 'aelfnikyqj', 4: 'Hush', 5: 'EnWei', 6: 'ShangHong', 7: 'Sean', 8: 'ZhiQing', 9: 'YiTing', 10: 'YuLiang', 11: 'cutestchiawen'}\n",
      "+++++++===\n",
      "./face/\n",
      "(9758, 64, 64, 3)\n",
      "[1 1 1 ... 2 2 2]\n",
      "4971\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9758, 4971]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e7a4045c42cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./face/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m     \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-e7a4045c42cb>\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, img_rows, img_cols, img_channels, nb_classes)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2125\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2127\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \"\"\"\n\u001b[0;32m    291\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9758, 4971]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D,GlobalAveragePooling2D,BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# import os\n",
    "# import sys\n",
    "# sys.path.insert(0, os.path.abspath('../final/'))\n",
    "\n",
    "# from load_dataset import load_dataset, resize_image, IMAGE_SIZE\n",
    "import Ipynb_importer\n",
    "from load_dataset import load_dataset, resize_image, IMAGE_SIZE\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, path_name):\n",
    "        #訓練集\n",
    "        self.train_images = None\n",
    "        self.train_labels = None\n",
    "        \n",
    "        #驗證集\n",
    "        self.valid_images = None\n",
    "        self.valid_labels = None\n",
    "        \n",
    "        #測試集\n",
    "        self.test_images  = None            \n",
    "        self.test_labels  = None\n",
    "        \n",
    "        #資料集載入路徑\n",
    "        self.path_name    = path_name\n",
    "        \n",
    "        #當前庫採用的維度順序\n",
    "        self.input_shape = None\n",
    "        \n",
    "    #載入資料集並按照交叉驗證的原則劃分資料集並進行相關預處理工作\n",
    "    def load(self, img_rows = IMAGE_SIZE, img_cols = IMAGE_SIZE, \n",
    "             img_channels = 3, nb_classes =4):\n",
    "        peoples = {}\n",
    "        with open('./faceDetail.csv', newline='') as csvfile:\n",
    "                  # 讀取 CSV 檔案內容\n",
    "            rows = csv.reader(csvfile)\n",
    "            for row in rows :\n",
    "                if(row[1] == 'label') :\n",
    "                    continue\n",
    "                peoples[int(row[1])] = row[0]\n",
    "        nb_classes = len(peoples)\n",
    "        print(peoples)\n",
    "        #載入資料集到記憶體\n",
    "        images, labels = load_dataset(self.path_name)        \n",
    "        \n",
    "        train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size = 0.2, random_state = random.randint(0, 100))                \n",
    "        train_images, valid_images, train_labels, valid_labels = train_test_split(train_images, train_labels, test_size = 0.3, random_state = random.randint(0, 100))        \n",
    "        \n",
    "        #當前的維度順序如果為'th'，則輸入圖片資料時的順序為：channels,rows,cols，否則:rows,cols,channels\n",
    "#                          channels_first                channels,width,height    width、height、channels\n",
    "        #這部分程式碼就是根據keras庫要求的維度順序重組訓練資料集\n",
    "        if K.image_data_format() == 'channels_first':# 高版本使用\n",
    "#         if K.image_dim_ordering() == 'th':# 低版本使用\n",
    "            train_images = train_images.reshape(train_images.shape[0], img_channels, img_rows, img_cols)\n",
    "            valid_images = valid_images.reshape(valid_images.shape[0], img_channels, img_rows, img_cols)\n",
    "            test_images = test_images.reshape(test_images.shape[0], img_channels, img_rows, img_cols)\n",
    "            self.input_shape = (img_channels, img_rows, img_cols)            \n",
    "        else:\n",
    "            print(img_rows, img_cols, img_channels);\n",
    "            train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            valid_images = valid_images.reshape(valid_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            self.input_shape = (img_rows, img_cols, img_channels)            \n",
    "            \n",
    "        #輸出訓練集、驗證集、測試集的數量 + 總數量\n",
    "        print(train_images.shape[0], 'train samples')\n",
    "        print(valid_images.shape[0], 'valid samples')\n",
    "        print(test_images.shape[0], 'test samples')\n",
    "        print(images.shape[0], 'total samples')\n",
    "        #我們的模型使用categorical_crossentropy作為損失函式，因此需要根據類別數量nb_classes將\n",
    "        #類別標籤進行one-hot編碼使其向量化，在這裡我們的類別只有兩種，經過轉化後標籤資料變為二維\n",
    "        train_labels = np_utils.to_categorical(train_labels, nb_classes)                        \n",
    "        valid_labels = np_utils.to_categorical(valid_labels, nb_classes)            \n",
    "        test_labels = np_utils.to_categorical(test_labels, nb_classes)                        \n",
    "\n",
    "        #畫素資料浮點化以便歸一化\n",
    "        train_images = train_images.astype('float32')            \n",
    "        valid_images = valid_images.astype('float32')\n",
    "        test_images = test_images.astype('float32')\n",
    "\n",
    "        #將其歸一化,影象的各畫素值歸一化到0~1區間\n",
    "        train_images /= 255\n",
    "        valid_images /= 255\n",
    "        test_images /= 255            \n",
    "\n",
    "        self.train_images = train_images\n",
    "        self.valid_images = valid_images\n",
    "        self.test_images  = test_images\n",
    "        self.train_labels = train_labels\n",
    "        self.valid_labels = valid_labels\n",
    "        self.test_labels  = test_labels\n",
    "            \n",
    "#CNN網路模型類            \n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.model = None \n",
    "        \n",
    "    #建立模型\n",
    "    def build_model(self, dataset, nb_classes = 4):\n",
    "        #構建一個空的網路模型，它是一個線性堆疊模型，各神經網路層會被順序新增，專業名稱為序貫模型或線性堆疊模型\n",
    "        self.model = Sequential() \n",
    "        peoples = {}\n",
    "        with open('./faceDetail.csv', newline='') as csvfile:\n",
    "                  # 讀取 CSV 檔案內容\n",
    "            rows = csv.reader(csvfile)\n",
    "            for row in rows :\n",
    "                if(row[1] == 'label') :\n",
    "                    continue\n",
    "                peoples[int(row[1])] = row[0]\n",
    "        nb_classes = len(peoples)\n",
    "        #以下程式碼將順序新增CNN網路需要的各層，一個add就是一個網路層\n",
    "        #                           (輸出維度,)\n",
    "          \n",
    "#         self.model.add(Convolution2D(32, 3, 3,padding='same', \n",
    "#                                      input_shape = dataset.input_shape))    #1 2維卷積層\n",
    "#         self.model.add(Activation('relu'))                                  #2 啟用函式層\n",
    "        self.model.add(ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape = dataset.input_shape))\n",
    "#         self.model.add(Activation('relu'))   \n",
    "#         self.model.add(Dropout(0.5))\n",
    "        \n",
    "#         self.model.add(Convolution2D(32, 3, 3))                             #3 2維卷積層                             \n",
    "#         self.model.add(Activation('relu'))                                  #4 啟用函式層\n",
    "        \n",
    "#         self.model.add(MaxPooling2D(pool_size=(2, 2)))                      #5 池化層\n",
    "#         self.model.add(Dropout(0.))                                       #6 Dropout層\n",
    "\n",
    "#         self.model.add(Convolution2D(64, 3, 3, padding='same'))         #7  2維卷積層\n",
    "#         self.model.add(Activation('relu'))                                  #8  啟用函式層\n",
    "        \n",
    "#         self.model.add(Convolution2D(64, 3, 3))                             #9  2維卷積層\n",
    "#         self.model.add(Activation('relu'))                                  #10 啟用函式層\n",
    "        \n",
    "#         self.model.add(MaxPooling2D(pool_size=(2, 2)))                      #11 池化層\n",
    "#         self.model.add(Dropout(0.25))                                       #12 Dropout層\n",
    "#         self.model.add(GlobalAveragePooling2D(name='average_pool'))\n",
    "        self.model.add(Flatten())                                           #13 Flatten層\n",
    "#         self.model.add(Dense(512, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.001)))\n",
    "        self.model.add(Dense(64))                                          #14 Dense層,又被稱作全連線層\n",
    "        self.model.add(Activation('relu'))                                  #15 啟用函式層   \n",
    "#         self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.5)) \n",
    "        self.model.add(Dense(64,activation='relu')) \n",
    "        self.model.add(Dropout(0.5))                                        #16 Dropout層\n",
    "#         \n",
    "        \n",
    "        self.model.add(Dense(nb_classes))                                   #17 Dense層\n",
    "        self.model.add(Activation('softmax'))                               #18 分類層，輸出最終結果\n",
    "        \n",
    "        self.model.summary()\n",
    "\n",
    "    \n",
    "\n",
    "    #訓練模型\n",
    "    def train(self, dataset, batch_size = 32, nb_epoch = 300, data_augmentation = False):\n",
    "        \n",
    "        learning_rate = 0.001\n",
    "#         decay_rate = learning_rate/nb_epoch\n",
    "        decay_rate = 1e-6\n",
    "        \n",
    "        self.model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=learning_rate,  decay=decay_rate, amsgrad=True), metrics=['acc'])\n",
    "#         self.model.compile(loss='categorical_crossentropy',optimizer=Adam(), metrics=['acc'])\n",
    "        #輸出模型概況\n",
    "#         sgd = SGD(lr = 0.01, decay = 1e-6, \n",
    "#                   momentum = 0.9, nesterov = True) #採用SGD+momentum的優化器進行訓練，首先生成一個優化器物件  \n",
    "#         self.model.compile(loss='categorical_crossentropy',\n",
    "#                            optimizer=sgd,\n",
    "#                            metrics=['accuracy'])   #完成實際的模型配置工作\n",
    "        \n",
    "        #不使用資料提升，所謂的提升就是從我們提供的訓練資料中利用旋轉、翻轉、加噪聲等方法創造新的\n",
    "        #訓練資料，有意識的提升訓練資料規模，增加模型訓練量\n",
    "        if not data_augmentation:            \n",
    "            history = self.model.fit(dataset.train_images,\n",
    "                           dataset.train_labels,\n",
    "                           batch_size = batch_size,\n",
    "                           epochs = nb_epoch,\n",
    "                           validation_data = (dataset.valid_images, dataset.valid_labels),\n",
    "                           shuffle = True,\n",
    "                           callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./log')])\n",
    "            #history列表\n",
    "            print(history.history.keys())\n",
    "            #accuracy的歷史\n",
    "            plt.plot(history.history['acc'])\n",
    "            plt.plot(history.history['val_acc'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "            #loss的歷史\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "            plt.show()\n",
    "        #使用實時資料提升\n",
    "        else:            \n",
    "            #定義資料生成器用於資料提升，其返回一個生成器物件datagen，datagen每被呼叫一\n",
    "            #次其生成一組資料（順序生成），節省記憶體，其實就是python的資料生成器\n",
    "            datagen = ImageDataGenerator(\n",
    "                featurewise_center = False,             #是否使輸入資料去中心化（均值為0），\n",
    "                samplewise_center  = False,             #是否使輸入資料的每個樣本均值為0\n",
    "                featurewise_std_normalization = False,  #是否資料標準化（輸入資料除以資料集的標準差）\n",
    "                samplewise_std_normalization  = False,  #是否將每個樣本資料除以自身的標準差\n",
    "                zca_whitening = False,                  #是否對輸入資料施以ZCA白化\n",
    "                rotation_range = 10,                    #資料提升時圖片隨機轉動的角度(範圍為0～180)\n",
    "                rescale = 1./255,\n",
    "                width_shift_range  = 0.1,               #資料提升時圖片水平偏移的幅度（單位為圖片寬度的佔比，0~1之間的浮點數）\n",
    "                height_shift_range = 0.1,               #同上，只不過這裡是垂直\n",
    "                horizontal_flip = False,                 #是否進行隨機水平翻轉\n",
    "                vertical_flip = False,                   #是否進行隨機垂直翻轉\n",
    "                fill_mode = 'constant')                 \n",
    "\n",
    "            #計算整個訓練樣本集的數量以用於特徵值歸一化、ZCA白化等處理\n",
    "            datagen.fit(dataset.train_images) \n",
    "            test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "            test_datagen.fit(dataset.valid_images)\n",
    "            print(dataset.train_images.shape[0])\n",
    "            #利用生成器開始訓練模型\n",
    "#             self.model.fit(dataset.train_images,\n",
    "#                            dataset.train_labels,\n",
    "#                            batch_size = batch_size,\n",
    "#                            epochs = nb_epoch,\n",
    "#                            validation_data = (dataset.valid_images, dataset.valid_labels),\n",
    "#                            shuffle = True)\n",
    "           \n",
    "#             early_stopping = EarlyStopping(monitor='val_loss',patience=100) \n",
    "#             reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "            self.model.fit(datagen.flow(dataset.train_images, dataset.train_labels,\n",
    "                                                   batch_size = batch_size),\n",
    "#                             steps_per_epoch = dataset.train_images.shape[0]/batch_size,\n",
    "                            epochs = nb_epoch,\n",
    "                            validation_data = test_datagen.flow(dataset.valid_images, dataset.valid_labels,batch_size = batch_size),\n",
    "                            shuffle = True,\n",
    "                            callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./log')])    \n",
    "#              self.model.fit(datagen.flow(dataset.train_images, dataset.train_labels,\n",
    "#                                                    batch_size = batch_size),\n",
    "# #                             steps_per_epoch = dataset.train_images.shape[0]/batch_size,\n",
    "#                             epochs = nb_epoch,\n",
    "#                             validation_data = (dataset.valid_images, dataset.valid_labels),\n",
    "#                             shuffle = True)    \n",
    "    \n",
    "    MODEL_PATH = './haarcascade.face.model.h5'\n",
    "    def save_model(self, file_path = MODEL_PATH):\n",
    "         self.model.save(file_path)\n",
    " \n",
    "    def load_model(self, file_path = MODEL_PATH):\n",
    "         self.model = load_model(file_path)\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "         score = self.model.evaluate(dataset.test_images, dataset.test_labels, verbose = 1)\n",
    "         print(\"%s: %.2f%%\" % (self.model.metrics_names[1], score[1] * 100))\n",
    "\n",
    "    #識別人臉\n",
    "    def face_predict(self, image):    \n",
    "        #依然是根據後端系統確定維度順序\n",
    "        if K.image_data_format() == 'channels_first' and image.shape != (1, 3, IMAGE_SIZE, IMAGE_SIZE):\n",
    "            image = resize_image(image)                             #尺寸必須與訓練集一致都應該是IMAGE_SIZE x IMAGE_SIZE\n",
    "            image = image.reshape((1, 3, IMAGE_SIZE, IMAGE_SIZE))   #與模型訓練不同，這次只是針對1張圖片進行預測    \n",
    "        elif K.image_data_format() == 'channels_last' and image.shape != (1, IMAGE_SIZE, IMAGE_SIZE, 3):\n",
    "            image = resize_image(image)\n",
    "            image = image.reshape((1, IMAGE_SIZE, IMAGE_SIZE, 3))                    \n",
    "        \n",
    "        #浮點並歸一化\n",
    "        image = image.astype('float32')\n",
    "        image /= 255\n",
    "        \n",
    "        #給出輸入屬於各個類別的概率，函式會給出輸入影象概率各為多少\n",
    "        result = self.model.predict_classes(image) \n",
    "        print('result:', self.model.predict(image))\n",
    "        print(\"===========face_predict==========\")\n",
    "        print(self.model.predict(image)[0][result[0]])\n",
    "#         result = self.model.predict(image)\n",
    "        print('result:', result)\n",
    "        possibility = self.model.predict(image)[0][result[0]]\n",
    "#         if(result)\n",
    "        #給出類別預測\n",
    "        if possibility > 0.9 :  \n",
    "            return result[0]\n",
    "        #返回類別預測結果\n",
    "        return -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     dataset = Dataset('./face/')    \n",
    "#     dataset.load()\n",
    "    \n",
    "#     model = Model()\n",
    "#     model.build_model(dataset)\n",
    "    \n",
    "#     #先前新增的測試build_model()函式的程式碼\n",
    "#     model.build_model(dataset)\n",
    "\n",
    "#     #測試訓練函式的程式碼\n",
    "#     model.train(dataset)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    dataset = Dataset('./face/')    \n",
    "    dataset.load()\n",
    "    \n",
    "    model = Model()\n",
    "    model.build_model(dataset)\n",
    "    print(\"======dataset======\")\n",
    "    print(dataset)\n",
    "    model.train(dataset)\n",
    "    model.save_model(file_path = './model/haarcascade.face.model.h5')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':    \n",
    "    dataset = Dataset('./face/')    \n",
    "    dataset.load()\n",
    "\n",
    "    \n",
    "    #評估模型\n",
    "    model = Model()\n",
    "    model.load_model(file_path = './model/haarcascade.face.model.h5')\n",
    "    model.evaluate(dataset)    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
